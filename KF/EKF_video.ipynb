{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cccba7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoImageProcessor, AutoModelForObjectDetection\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1229\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1463\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1461\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1462\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m-> 1463\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1462\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1460\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m   1461\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1462\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1463\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1472\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1470\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1471\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1472\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1473\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1474\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1475\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1476\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1477\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\image_processing_auto.py:26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamic_module_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_class_from_dynamic_module, resolve_trust_remote_code\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_processing_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageProcessingMixin\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CONFIG_NAME, IMAGE_PROCESSOR_NAME, get_file_from_repo, logging\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto_factory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _LazyAutoMapping\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\image_processing_utils.py:28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamic_module_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m custom_object_save\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchFeature \u001b[38;5;28;01mas\u001b[39;00m BaseBatchFeature\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_transforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m center_crop, normalize, rescale\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChannelDimension\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     31\u001b[0m     IMAGE_PROCESSOR_NAME,\n\u001b[0;32m     32\u001b[0m     PushToHubMixin,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m     logging,\n\u001b[0;32m     41\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\image_transforms.py:47\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_flax_available():\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjnp\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\__init__.py:49\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[0;32m     47\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__ namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\autograph\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.autograph namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mag_ctx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_status_ctx \u001b[38;5;66;03m# line: 34\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_convert \u001b[38;5;66;03m# line: 493\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\autograph\\core\\ag_ctx.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ag_logging\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[0;32m     25\u001b[0m stacks \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mlocal()\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\autograph\\utils\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Utility module that contains APIs usable in the generated code.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext_managers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_dependency_on_returns\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m alias_tensors\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor_list\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dynamic_list_append\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\autograph\\utils\\context_managers.py:20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_array_ops\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontrol_dependency_on_returns\u001b[39m(return_value):\n\u001b[0;32m     24\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Create a TF control dependency on the return values of a function.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03m  If the function had no return value, a no-op context is returned.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m    A context manager.\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\tensor_array_ops.py:42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_data_flow_ops\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m list_ops\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m math_ops\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_logging \u001b[38;5;28;01mas\u001b[39;00m logging\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nested_structure_coder\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:94\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_nn_ops\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_sparse_ops\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_math_operator_overrides  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgen_math_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:936\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1032\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1131\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 객체 클래스 탐색\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from transformers import AutoImageProcessor, AutoModelForObjectDetection\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# --- 설정 변수 ---\n",
    "VIDEO_INPUT_PATH = \"KF_vid3.mp4\"\n",
    "VIDEO_OUTPUT_PATH = \"detection_check_output.mp4\"\n",
    "HF_MODEL_NAME = \"facebook/detr-resnet-50\"\n",
    "CONFIDENCE_THRESHOLD = 0.7  # 임계값을 조절하여 탐지 결과를 필터링할 수 있습니다.\n",
    "\n",
    "# --- Hugging Face 모델 및 프로세서 로드 ---\n",
    "try:\n",
    "    image_processor = AutoImageProcessor.from_pretrained(HF_MODEL_NAME)\n",
    "    model = AutoModelForObjectDetection.from_pretrained(HF_MODEL_NAME)\n",
    "    print(f\"Hugging Face 모델 '{HF_MODEL_NAME}' 로드 성공.\")\n",
    "except Exception as e:\n",
    "    print(f\"오류: Hugging Face 모델 '{HF_MODEL_NAME}' 로드 실패. ({e})\")\n",
    "    exit()\n",
    "\n",
    "def detect_and_draw_all_objects(frame_color_cv):\n",
    "    \"\"\"\n",
    "    프레임 내에서 신뢰도 임계값을 넘는 모든 객체를 탐지하고,\n",
    "    바운딩 박스와 클래스 이름을 프레임에 직접 그립니다.\n",
    "    탐지된 모든 클래스 이름의 집합(set)을 반환합니다.\n",
    "    \"\"\"\n",
    "    image_pil = Image.fromarray(cv2.cvtColor(frame_color_cv, cv2.COLOR_BGR2RGB))\n",
    "    inputs = image_processor(images=image_pil, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    target_sizes = torch.tensor([image_pil.size[::-1]])\n",
    "    results = image_processor.post_process_object_detection(outputs,\n",
    "                                                             threshold=CONFIDENCE_THRESHOLD,\n",
    "                                                             target_sizes=target_sizes)[0]\n",
    "\n",
    "    detected_classes_in_frame = set()\n",
    "\n",
    "    # 탐지된 모든 객체에 대해 루프를 돕니다.\n",
    "    for score, label_id, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "        # 클래스 이름 가져오기\n",
    "        label_name = model.config.id2label[label_id.item()]\n",
    "        detected_classes_in_frame.add(label_name)\n",
    "\n",
    "        # 바운딩 박스 좌표 계산\n",
    "        startX, startY, endX, endY = map(int, box.tolist())\n",
    "\n",
    "        # 프레임에 바운딩 박스 그리기 (클래스별로 다른 색상)\n",
    "        color = np.random.randint(0, 255, size=3).tolist()\n",
    "        cv2.rectangle(frame_color_cv, (startX, startY), (endX, endY), color, 2)\n",
    "\n",
    "        # 클래스 이름과 신뢰도 점수 표시\n",
    "        text = f\"{label_name}: {score:.2f}\"\n",
    "        cv2.putText(frame_color_cv, text, (startX, startY - 10), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "    return detected_classes_in_frame\n",
    "\n",
    "# ==============================================================\n",
    "#  메인 실행 루프\n",
    "# ==============================================================\n",
    "cap = cv2.VideoCapture(VIDEO_INPUT_PATH)\n",
    "if not cap.isOpened():\n",
    "    print(f\"오류: '{VIDEO_INPUT_PATH}' 영상을 열 수 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "# 비디오 저장 설정\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out_video = cv2.VideoWriter(VIDEO_OUTPUT_PATH, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "print(\"영상에서 탐지되는 모든 클래스 확인을 시작합니다...\")\n",
    "\n",
    "frame_idx = 0\n",
    "last_detected_classes = set()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    output_frame = frame.copy()\n",
    "\n",
    "    # 현재 프레임에서 모든 객체 탐지 및 그리기\n",
    "    detected_classes = detect_and_draw_all_objects(output_frame)\n",
    "\n",
    "    # 콘솔에 탐지된 클래스 목록 출력 (이전 프레임과 다를 경우에만)\n",
    "    if detected_classes and detected_classes != last_detected_classes:\n",
    "        print(f\"[프레임 {frame_idx}] 탐지된 클래스: {sorted(list(detected_classes))}\")\n",
    "        last_detected_classes = detected_classes\n",
    "    elif not detected_classes and last_detected_classes:\n",
    "        print(f\"[프레임 {frame_idx}] 탐지된 클래스 없음\")\n",
    "        last_detected_classes = set()\n",
    "\n",
    "\n",
    "    # 화면 좌측 상단에 현재 프레임에서 탐지된 클래스 목록 표시\n",
    "    y_offset = 30\n",
    "    cv2.putText(output_frame, \"Detected Classes:\", (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n",
    "    for cls_name in sorted(list(detected_classes)):\n",
    "        y_offset += 25\n",
    "        cv2.putText(output_frame, f\"- {cls_name}\", (15, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n",
    "\n",
    "\n",
    "    # 결과 비디오에 프레임 쓰기\n",
    "    out_video.write(output_frame)\n",
    "    frame_idx += 1\n",
    "\n",
    "# --- 종료 ---\n",
    "cap.release()\n",
    "out_video.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"\\n작업 완료.\")\n",
    "print(f\"탐지 결과가 표시된 비디오가 다음 경로에 저장되었습니다: {VIDEO_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5209026f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face 모델 로드 성공.\n",
      "단계 1: 비디오 및 EKF 초기화 중...\n",
      "단계 1 완료.\n",
      "단계 2: 영상 처리 및 EKF 적용 시작...\n",
      "프레임 4에서 EKF 초기화. 추정된 초기 깊이 Z = 3.13 m\n",
      "프레임 30 처리 중...\n",
      "프레임 60 처리 중...\n",
      "프레임 90 처리 중...\n",
      "프레임 120 처리 중...\n",
      "프레임 150 처리 중...\n",
      "프레임 180 처리 중...\n",
      "프레임 210 처리 중...\n",
      "프레임 240 처리 중...\n",
      "프레임 270 처리 중...\n",
      "프레임 300 처리 중...\n",
      "프레임 330 처리 중...\n",
      "프레임 360 처리 중...\n",
      "프레임 390 처리 중...\n",
      "단계 4: 처리 완료 및 종료.\n",
      "EKF 처리 완료. 결과 비디오가 다음 경로에 저장되었습니다: bowling_P_o.mp4\n"
     ]
    }
   ],
   "source": [
    "# 핀홀 카메라 등속도 모델\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from transformers import AutoImageProcessor, AutoModelForObjectDetection\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# --- 설정 변수 ---\n",
    "VIDEO_INPUT_PATH = \"bowling.mp4\" # 분석한 영상 파일명으로 변경\n",
    "VIDEO_OUTPUT_PATH = \"bowling_P_o.mp4\"\n",
    "USER_TARGET_CLASS_NAME = 'sports ball' # ★★★ 타겟 변경\n",
    "HF_MODEL_NAME = \"facebook/detr-resnet-50\"\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "\n",
    "# --- EKF를 위한 물리 파라미터 ---\n",
    "FOCAL_LENGTH_PIXELS = 1200\n",
    "REAL_OBJECT_WIDTH_M = 0.22\n",
    "REAL_OBJECT_HEIGHT_M = 0.22\n",
    "\n",
    "# --- 시스템 상태 정의 ---\n",
    "# <<< [OPTIMIZED] 반응성 개선을 위해 노이즈 파라미터 대폭 수정 >>>\n",
    "# Q: 프로세스 노이즈. 모델 예측의 불확실성. 값을 높여 반응성을 키움.\n",
    "q_pos_std = 5.0   # 픽셀 속도 변화의 불확실성 증가 (기존 0.1)\n",
    "q_depth_std = 10.0 # 깊이 속도 변화의 불확실성 대폭 증가 (기존 1.0)\n",
    "Q = np.diag([0, 0, 0, q_pos_std**2, q_pos_std**2, q_depth_std**2])\n",
    "\n",
    "# R: 측정 노이즈. 측정값의 불확실성. 값을 낮춰 측정값을 더 신뢰하게 함.\n",
    "r_pos_std = 3.0 # 픽셀 측정 정확도 신뢰도 증가 (기존 5.0)\n",
    "r_size_std = 5.0 # 크기 측정 정확도 신뢰도 증가 (기존 10.0)\n",
    "R = np.diag([r_pos_std**2, r_pos_std**2, r_size_std**2, r_size_std**2])\n",
    "\n",
    "# --- Hugging Face 모델 및 프로세서 로드 ---\n",
    "try:\n",
    "    image_processor = AutoImageProcessor.from_pretrained(HF_MODEL_NAME)\n",
    "    model = AutoModelForObjectDetection.from_pretrained(HF_MODEL_NAME)\n",
    "    print(\"Hugging Face 모델 로드 성공.\")\n",
    "except Exception as e:\n",
    "    print(f\"오류: Hugging Face 모델 로드 실패. ({e})\")\n",
    "    exit()\n",
    "\n",
    "# --- EKF 관련 함수 정의 ---\n",
    "def h_measurement_function(x_state):\n",
    "    cx, cy, Z, _, _, _ = x_state\n",
    "    \n",
    "    # <<< [OPTIMIZED] 필터 안정성 강화를 위한 Z값 하한 설정 >>>\n",
    "    Z = np.clip(Z, 0.1, 1000) # Z가 최소 10cm 이상, 최대 1km 라고 제한\n",
    "\n",
    "    w_pred = (FOCAL_LENGTH_PIXELS * REAL_OBJECT_WIDTH_M) / Z\n",
    "    h_pred = (FOCAL_LENGTH_PIXELS * REAL_OBJECT_HEIGHT_M) / Z\n",
    "    \n",
    "    return np.array([cx, cy, w_pred, h_pred])\n",
    "\n",
    "def calculate_jacobian_Hj(x_eval):\n",
    "    _, _, Z, _, _, _ = x_eval\n",
    "\n",
    "    # <<< [OPTIMIZED] 필터 안정성 강화를 위한 Z값 하한 설정 >>>\n",
    "    Z = np.clip(Z, 0.1, 1000)\n",
    "    \n",
    "    H_j = np.zeros((4, 6))\n",
    "    H_j[0, 0] = 1\n",
    "    H_j[1, 1] = 1\n",
    "    H_j[2, 2] = -(FOCAL_LENGTH_PIXELS * REAL_OBJECT_WIDTH_M) / (Z**2)\n",
    "    H_j[3, 2] = -(FOCAL_LENGTH_PIXELS * REAL_OBJECT_HEIGHT_M) / (Z**2)\n",
    "    \n",
    "    return H_j\n",
    "\n",
    "# (객체 탐지 함수는 변경 없음)\n",
    "def detect_object_huggingface(frame_color_cv):\n",
    "    image_pil = Image.fromarray(cv2.cvtColor(frame_color_cv, cv2.COLOR_BGR2RGB))\n",
    "    inputs = image_processor(images=image_pil, return_tensors=\"pt\")\n",
    "    with torch.no_grad(): outputs = model(**inputs)\n",
    "    target_sizes = torch.tensor([image_pil.size[::-1]])\n",
    "    results = image_processor.post_process_object_detection(outputs, threshold=CONFIDENCE_THRESHOLD, target_sizes=target_sizes)[0]\n",
    "    best_target_detection = None\n",
    "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "        if model.config.id2label[label.item()].lower() == USER_TARGET_CLASS_NAME:\n",
    "            box = [round(i, 2) for i in box.tolist()]\n",
    "            w, h = box[2] - box[0], box[3] - box[1]\n",
    "            cx, cy = box[0] + w/2, box[1] + h/2\n",
    "            current_score = score.item()\n",
    "            if best_target_detection is None or current_score > best_target_detection[0]:\n",
    "                 best_target_detection = (current_score, np.array([cx, cy, w, h]))\n",
    "    return best_target_detection[1] if best_target_detection else None\n",
    "\n",
    "# ==============================================================\n",
    "# 단계 1: 비디오 및 EKF 초기화\n",
    "# ==============================================================\n",
    "print(\"단계 1: 비디오 및 EKF 초기화 중...\")\n",
    "cap = cv2.VideoCapture(VIDEO_INPUT_PATH)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) if cap.get(cv2.CAP_PROP_FPS) > 0 else 30\n",
    "dt = 1.0 / fps\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out_video = cv2.VideoWriter(VIDEO_OUTPUT_PATH, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "A = np.eye(6)\n",
    "A[0, 3], A[1, 4], A[2, 5] = dt, dt, dt\n",
    "\n",
    "x_est = np.zeros(6)\n",
    "P = np.eye(6) * 1000.0\n",
    "is_initialized = False\n",
    "print(\"단계 1 완료.\")\n",
    "\n",
    "# ==============================================================\n",
    "# 단계 2: 영상 처리 및 EKF 적용 루프\n",
    "# ==============================================================\n",
    "print(\"단계 2: 영상 처리 및 EKF 적용 시작...\")\n",
    "frame_idx = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "    output_frame = frame.copy()\n",
    "    \n",
    "    measurement_k = detect_object_huggingface(frame)\n",
    "\n",
    "    if not is_initialized and measurement_k is not None:\n",
    "        cx_meas, cy_meas, w_meas, h_meas = measurement_k\n",
    "        if h_meas > 1: # <<< [OPTIMIZED] 너무 작은 박스는 초기화에 사용하지 않음\n",
    "            initial_Z = (FOCAL_LENGTH_PIXELS * REAL_OBJECT_HEIGHT_M) / h_meas\n",
    "            x_est[0], x_est[1], x_est[2] = cx_meas, cy_meas, initial_Z\n",
    "            P = np.eye(6) * 100.0\n",
    "            is_initialized = True\n",
    "            print(f\"프레임 {frame_idx}에서 EKF 초기화. 추정된 초기 깊이 Z = {initial_Z:.2f} m\")\n",
    "\n",
    "    if is_initialized:\n",
    "        x_pred = A @ x_est\n",
    "        P_pred = A @ P @ A.T + Q\n",
    "\n",
    "        if measurement_k is not None:\n",
    "            H_j_k = calculate_jacobian_Hj(x_pred)\n",
    "            z_pred = h_measurement_function(x_pred)\n",
    "            y = measurement_k - z_pred\n",
    "            S = H_j_k @ P_pred @ H_j_k.T + R\n",
    "            K = P_pred @ H_j_k.T @ np.linalg.inv(S)\n",
    "            x_est = x_pred + K @ y\n",
    "            P = (np.eye(6) - K @ H_j_k) @ P_pred\n",
    "        else:\n",
    "            x_est = x_pred\n",
    "            P = P_pred\n",
    "\n",
    "    # ==============================================================\n",
    "    # 단계 3: 시각화\n",
    "    # ==============================================================\n",
    "    if measurement_k is not None:\n",
    "        cx, cy, w, h = measurement_k\n",
    "        cv2.rectangle(output_frame, (int(cx-w/2), int(cy-h/2)), (int(cx+w/2), int(cy+h/2)), (0, 255, 0), 2)\n",
    "        cv2.putText(output_frame, \"Detection\", (int(cx-w/2), int(cy-h/2)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "    if is_initialized:\n",
    "        est_box_params = h_measurement_function(x_est)\n",
    "        cx, cy, w, h = est_box_params\n",
    "        est_Z = x_est[2]\n",
    "        cv2.rectangle(output_frame, (int(cx-w/2), int(cy-h/2)), (int(cx+w/2), int(cy+h/2)), (0, 0, 255), 2)\n",
    "        cv2.putText(output_frame, f\"EKF (Z:{est_Z:.1f}m)\", (int(cx-w/2), int(cy-h/2)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "\n",
    "    out_video.write(output_frame)\n",
    "    frame_idx += 1\n",
    "    if frame_idx % 30 == 0: print(f\"프레임 {frame_idx} 처리 중...\")\n",
    "\n",
    "# ==============================================================\n",
    "# 단계 4: 종료\n",
    "# ==============================================================\n",
    "print(\"단계 4: 처리 완료 및 종료.\")\n",
    "cap.release()\n",
    "out_video.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(f\"EKF 처리 완료. 결과 비디오가 다음 경로에 저장되었습니다: {VIDEO_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf29aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 핀홀 카메라 등가속도 모델\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from transformers import AutoImageProcessor, AutoModelForObjectDetection\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# --- 설정 변수 ---\n",
    "VIDEO_INPUT_PATH = \"KF_vid2.mp4\" # 분석한 영상 파일명으로 변경\n",
    "VIDEO_OUTPUT_PATH = \"EKF_vid2_o2.mp4\"\n",
    "USER_TARGET_CLASS_NAME = 'sports ball'\n",
    "HF_MODEL_NAME = \"facebook/detr-resnet-50\"\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "\n",
    "# --- EKF를 위한 물리 파라미터 ---\n",
    "FOCAL_LENGTH_PIXELS = 1200\n",
    "REAL_OBJECT_WIDTH_M = 0.22\n",
    "REAL_OBJECT_HEIGHT_M = 0.22\n",
    "\n",
    "# --- 시스템 상태 정의 (9-DOF) ---\n",
    "# 상태 변수 x = [cx, cy, Z, v_cx, v_cy, v_Z, a_cx, a_cy, a_Z]' (9x1)\n",
    "# 측정값 z = [cx, cy, w, h]' (4x1)\n",
    "\n",
    "# <<< [9-DOF] Q: 프로세스 노이즈. 가속도의 불확실성(Jerk)에 해당. >>>\n",
    "# 이 값을 조절하여 가속도 변화에 대한 민감도를 튜닝.\n",
    "q_accel_pos_std = 50.0  # 픽셀 가속도 변화의 불확실성 (더 높은 값으로 시작)\n",
    "q_accel_depth_std = 100.0 # 깊이 가속도 변화의 불확실성 (더 높은 값으로 시작)\n",
    "Q = np.diag([0, 0, 0, 0, 0, 0, q_accel_pos_std**2, q_accel_pos_std**2, q_accel_depth_std**2])\n",
    "\n",
    "# R: 측정 노이즈. 이전과 동일.\n",
    "r_pos_std = 3.0\n",
    "r_size_std = 5.0\n",
    "R = np.diag([r_pos_std**2, r_pos_std**2, r_size_std**2, r_size_std**2])\n",
    "\n",
    "# --- Hugging Face 모델 로드 (변경 없음) ---\n",
    "try:\n",
    "    image_processor = AutoImageProcessor.from_pretrained(HF_MODEL_NAME)\n",
    "    model = AutoModelForObjectDetection.from_pretrained(HF_MODEL_NAME)\n",
    "    print(\"Hugging Face 모델 로드 성공.\")\n",
    "except Exception as e: exit(f\"모델 로드 실패: {e}\")\n",
    "\n",
    "# --- EKF 관련 함수 정의 ---\n",
    "def h_measurement_function(x_state_9d):\n",
    "    # <<< [9-DOF] 9차원 상태 벡터를 입력받지만, 위치 항만 사용 >>>\n",
    "    cx, cy, Z = x_state_9d[0], x_state_9d[1], x_state_9d[2]\n",
    "    Z = np.clip(Z, 0.1, 1000)\n",
    "    w_pred = (FOCAL_LENGTH_PIXELS * REAL_OBJECT_WIDTH_M) / Z\n",
    "    h_pred = (FOCAL_LENGTH_PIXELS * REAL_OBJECT_HEIGHT_M) / Z\n",
    "    return np.array([cx, cy, w_pred, h_pred])\n",
    "\n",
    "def calculate_jacobian_Hj(x_eval_9d):\n",
    "    # <<< [9-DOF] 자코비안 행렬이 4x9 차원으로 확장 >>>\n",
    "    Z = x_eval_9d[2]\n",
    "    Z = np.clip(Z, 0.1, 1000)\n",
    "    H_j = np.zeros((4, 9)) # 4x9 행렬\n",
    "    H_j[0, 0] = 1\n",
    "    H_j[1, 1] = 1\n",
    "    H_j[2, 2] = -(FOCAL_LENGTH_PIXELS * REAL_OBJECT_WIDTH_M) / (Z**2)\n",
    "    H_j[3, 2] = -(FOCAL_LENGTH_PIXELS * REAL_OBJECT_HEIGHT_M) / (Z**2)\n",
    "    return H_j\n",
    "\n",
    "# (객체 탐지 함수는 변경 없음)\n",
    "def detect_object_huggingface(frame_color_cv):\n",
    "    image_pil = Image.fromarray(cv2.cvtColor(frame_color_cv, cv2.COLOR_BGR2RGB))\n",
    "    inputs = image_processor(images=image_pil, return_tensors=\"pt\")\n",
    "    with torch.no_grad(): outputs = model(**inputs)\n",
    "    target_sizes = torch.tensor([image_pil.size[::-1]])\n",
    "    results = image_processor.post_process_object_detection(outputs, threshold=CONFIDENCE_THRESHOLD, target_sizes=target_sizes)[0]\n",
    "    best_target_detection = None\n",
    "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "        if model.config.id2label[label.item()].lower() == USER_TARGET_CLASS_NAME:\n",
    "            box = [round(i, 2) for i in box.tolist()]\n",
    "            w, h = box[2] - box[0], box[3] - box[1]\n",
    "            cx, cy = box[0] + w/2, box[1] + h/2\n",
    "            current_score = score.item()\n",
    "            if best_target_detection is None or current_score > best_target_detection[0]:\n",
    "                 best_target_detection = (current_score, np.array([cx, cy, w, h]))\n",
    "    return best_target_detection[1] if best_target_detection else None\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# 단계 1: 비디오 및 EKF 초기화\n",
    "# ==============================================================\n",
    "print(\"단계 1: 비디오 및 EKF 초기화 중...\")\n",
    "cap = cv2.VideoCapture(VIDEO_INPUT_PATH)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) if cap.get(cv2.CAP_PROP_FPS) > 0 else 30\n",
    "dt = 1.0 / fps\n",
    "frame_width, frame_height = int(cap.get(3)), int(cap.get(4))\n",
    "out_video = cv2.VideoWriter(VIDEO_OUTPUT_PATH, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "# <<< [9-DOF] 상태 전이 행렬 A를 9x9로 정의 >>>\n",
    "A = np.eye(9)\n",
    "for i in range(3):\n",
    "    A[i, i+3] = dt\n",
    "    A[i, i+6] = 0.5 * dt**2\n",
    "    A[i+3, i+6] = dt\n",
    "\n",
    "x_est = np.zeros(9) # 9차원 상태 벡터\n",
    "P = np.eye(9) * 1000.0 # 9x9 공분산 행렬\n",
    "is_initialized = False\n",
    "print(\"단계 1 완료.\")\n",
    "\n",
    "# ==============================================================\n",
    "# 단계 2: 영상 처리 및 EKF 적용 루프\n",
    "# ==============================================================\n",
    "print(\"단계 2: 영상 처리 및 EKF 적용 시작...\")\n",
    "frame_idx = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "    output_frame = frame.copy()\n",
    "    \n",
    "    measurement_k = detect_object_huggingface(frame)\n",
    "\n",
    "    if not is_initialized and measurement_k is not None:\n",
    "        cx_meas, cy_meas, w_meas, h_meas = measurement_k\n",
    "        if h_meas > 1:\n",
    "            initial_Z = (FOCAL_LENGTH_PIXELS * REAL_OBJECT_HEIGHT_M) / h_meas\n",
    "            # <<< [9-DOF] 9차원 상태 벡터 초기화 >>>\n",
    "            x_est[0], x_est[1], x_est[2] = cx_meas, cy_meas, initial_Z\n",
    "            # 속도, 가속도는 0으로 시작\n",
    "            P = np.eye(9) * 100.0\n",
    "            is_initialized = True\n",
    "            print(f\"프레임 {frame_idx}에서 9-DOF EKF 초기화. 추정된 초기 깊이 Z = {initial_Z:.2f} m\")\n",
    "\n",
    "    if is_initialized:\n",
    "        # EKF 예측 단계\n",
    "        x_pred = A @ x_est\n",
    "        P_pred = A @ P @ A.T + Q\n",
    "\n",
    "        # EKF 갱신 단계\n",
    "        if measurement_k is not None:\n",
    "            H_j_k = calculate_jacobian_Hj(x_pred)\n",
    "            z_pred = h_measurement_function(x_pred)\n",
    "            y = measurement_k - z_pred\n",
    "            S = H_j_k @ P_pred @ H_j_k.T + R\n",
    "            K = P_pred @ H_j_k.T @ np.linalg.inv(S)\n",
    "            x_est = x_pred + K @ y\n",
    "            P = (np.eye(9) - K @ H_j_k) @ P_pred\n",
    "        else:\n",
    "            x_est = x_pred\n",
    "            P = P_pred\n",
    "\n",
    "    # ==============================================================\n",
    "    # 단계 3: 시각화\n",
    "    # ==============================================================\n",
    "    if measurement_k is not None:\n",
    "        cx, cy, w, h = measurement_k\n",
    "        cv2.rectangle(output_frame, (int(cx-w/2), int(cy-h/2)), (int(cx+w/2), int(cy+h/2)), (0, 255, 0), 1)\n",
    "        # cv2.putText(output_frame, \"Detection\", (int(cx-w/2), int(cy-h/2)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "    if is_initialized:\n",
    "        est_box_params = h_measurement_function(x_est)\n",
    "        cx, cy, w, h = est_box_params\n",
    "        est_Z = x_est[2]\n",
    "        # <<< [9-DOF] 추정된 가속도 값 시각화 >>>\n",
    "        est_a_cx, est_a_cy, est_a_Z = x_est[6], x_est[7], x_est[8]\n",
    "        \n",
    "        cv2.rectangle(output_frame, (int(cx-w/2), int(cy-h/2)), (int(cx+w/2), int(cy+h/2)), (0, 0, 255), 2)\n",
    "        info_text = f\"EKF Z:{est_Z:.1f}m\"\n",
    "        accel_text = f\"a(px/s2):({int(est_a_cx)}, {int(est_a_cy)}) aZ:{est_a_Z:.1f}\"\n",
    "        cv2.putText(output_frame, info_text, (int(cx-w/2), int(cy-h/2)-25), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "        cv2.putText(output_frame, accel_text, (int(cx-w/2), int(cy-h/2)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 255), 1)\n",
    "\n",
    "\n",
    "    out_video.write(output_frame)\n",
    "    frame_idx += 1\n",
    "    if frame_idx % 30 == 0: print(f\"프레임 {frame_idx} 처리 중...\")\n",
    "\n",
    "# 단계 4: 종료\n",
    "print(\"단계 4: 처리 완료 및 종료.\")\n",
    "cap.release(), out_video.release(), cv2.destroyAllWindows()\n",
    "print(f\"9-DOF EKF 처리 완료. 결과: {VIDEO_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32633df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face 모델 로드 성공.\n",
      "단계 1: 비디오 및 EKF 초기화 중...\n",
      "단계 1 완료.\n",
      "단계 2: 영상 처리 및 EKF 적용 시작...\n",
      "프레임 0에서 EKF 초기화. 초기 깊이 Z = 4.49 m\n",
      "프레임 30 처리 중...\n",
      "프레임 60 처리 중...\n",
      "프레임 90 처리 중...\n",
      "프레임 120 처리 중...\n",
      "프레임 150 처리 중...\n",
      "프레임 180 처리 중...\n",
      "프레임 210 처리 중...\n",
      "프레임 240 처리 중...\n",
      "프레임 270 처리 중...\n",
      "프레임 300 처리 중...\n",
      "프레임 330 처리 중...\n",
      "프레임 360 처리 중...\n",
      "프레임 390 처리 중...\n",
      "프레임 420 처리 중...\n",
      "프레임 450 처리 중...\n",
      "프레임 480 처리 중...\n",
      "프레임 510 처리 중...\n",
      "프레임 540 처리 중...\n",
      "프레임 570 처리 중...\n",
      "프레임 600 처리 중...\n",
      "프레임 630 처리 중...\n",
      "프레임 660 처리 중...\n",
      "프레임 690 처리 중...\n",
      "프레임 720 처리 중...\n",
      "프레임 750 처리 중...\n",
      "프레임 780 처리 중...\n",
      "단계 4: 처리 완료 및 종료.\n",
      "공기 저항 모델 EKF 처리 완료. 결과: EKF_vid2_Drag.mp4\n"
     ]
    }
   ],
   "source": [
    "# 핀홀 카메라 등가속도 + 공기저항 모델\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from transformers import AutoImageProcessor, AutoModelForObjectDetection\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# --- 설정 변수 ---\n",
    "VIDEO_INPUT_PATH = \"KF_vid2.mp4\"\n",
    "VIDEO_OUTPUT_PATH = \"EKF_vid2_Drag.mp4\"\n",
    "USER_TARGET_CLASS_NAME = 'sports ball'\n",
    "HF_MODEL_NAME = \"facebook/detr-resnet-50\"\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "\n",
    "# --- EKF를 위한 물리 파라미터 ---\n",
    "FOCAL_LENGTH_PIXELS = 1200\n",
    "REAL_OBJECT_WIDTH_M = 0.22\n",
    "REAL_OBJECT_HEIGHT_M = 0.22\n",
    "K_DRAG = 0.001  # <<< [DRAG] 공기 저항 계수 (튜닝 필요)\n",
    "\n",
    "# --- 시스템 상태 정의 (9-DOF) ---\n",
    "q_accel_pos_std = 50.0\n",
    "q_accel_depth_std = 100.0\n",
    "Q = np.diag([0, 0, 0, 0, 0, 0, q_accel_pos_std**2, q_accel_pos_std**2, q_accel_depth_std**2])\n",
    "\n",
    "r_pos_std = 3.0\n",
    "r_size_std = 5.0\n",
    "R = np.diag([r_pos_std**2, r_pos_std**2, r_size_std**2, r_size_std**2])\n",
    "\n",
    "# --- Hugging Face 모델 로드 (변경 없음) ---\n",
    "try:\n",
    "    image_processor = AutoImageProcessor.from_pretrained(HF_MODEL_NAME)\n",
    "    model = AutoModelForObjectDetection.from_pretrained(HF_MODEL_NAME)\n",
    "    print(\"Hugging Face 모델 로드 성공.\")\n",
    "except Exception as e: exit(f\"모델 로드 실패: {e}\")\n",
    "\n",
    "# --- EKF 관련 함수 정의 ---\n",
    "def state_transition_function_f(x_prev, dt, k_d):\n",
    "    \"\"\" <<< [DRAG] 공기 저항을 포함한 비선형 상태 전이 함수 \"\"\"\n",
    "    px, py, Z, vx, vy, vZ, ax, ay, aZ = x_prev\n",
    "    \n",
    "    # 3D 속도 벡터 및 속력 계산\n",
    "    v_vec = np.array([vx, vy, vZ])\n",
    "    speed = np.linalg.norm(v_vec)\n",
    "\n",
    "    # 공기 저항에 의한 가속도 계산 (a_drag = -k_d * |v| * v)\n",
    "    a_drag = -k_d * speed * v_vec\n",
    "    \n",
    "    # 최종 가속도 = 기본 가속도 + 저항 가속도\n",
    "    ax_net = ax + a_drag[0]\n",
    "    ay_net = ay + a_drag[1]\n",
    "    aZ_net = aZ + a_drag[2]\n",
    "\n",
    "    # 등가속도 운동 공식 적용\n",
    "    px_next = px + vx * dt + 0.5 * ax_net * dt**2\n",
    "    py_next = py + vy * dt + 0.5 * ay_net * dt**2\n",
    "    Z_next = Z + vZ * dt + 0.5 * aZ_net * dt**2\n",
    "    \n",
    "    vx_next = vx + ax_net * dt\n",
    "    vy_next = vy + ay_net * dt\n",
    "    vZ_next = vZ + aZ_net * dt\n",
    "    \n",
    "    # 기본 가속도는 변하지 않는다고 가정\n",
    "    ax_next, ay_next, aZ_next = ax, ay, aZ\n",
    "    \n",
    "    return np.array([px_next, py_next, Z_next, vx_next, vy_next, vZ_next, ax_next, ay_next, aZ_next])\n",
    "\n",
    "def calculate_jacobian_F(x_eval, dt, k_d):\n",
    "    \"\"\" <<< [DRAG] 비선형 상태 전이 함수 f(x)의 자코비안 F_k 계산 \"\"\"\n",
    "    _, _, _, vx, vy, vZ, _, _, _ = x_eval\n",
    "    \n",
    "    F = np.eye(9)\n",
    "    # 위치, 속도, 가속도 간의 기본 관계 설정\n",
    "    for i in range(3):\n",
    "        F[i, i+3] = dt\n",
    "        F[i, i+6] = 0.5 * dt**2\n",
    "        F[i+3, i+6] = dt\n",
    "\n",
    "    # 공기 저항 항의 편미분 추가\n",
    "    s = np.sqrt(vx**2 + vy**2 + vZ**2)\n",
    "    v = {'x': vx, 'y': vy, 'z': vZ}\n",
    "    dims = ['x', 'y', 'z']\n",
    "    \n",
    "    epsilon = 1e-6\n",
    "    if s > epsilon:\n",
    "        for i, dim1 in enumerate(dims): # a_net_dim1\n",
    "            for j, dim2 in enumerate(dims): # v_dim2\n",
    "                # d(a_drag_i)/d(v_j)\n",
    "                term1 = -k_d * (v[dim1] * v[dim2] / s)\n",
    "                if i == j:\n",
    "                    term1 += -k_d * s\n",
    "                \n",
    "                # F의 해당 위치에 편미분 값 추가\n",
    "                # d(p_i)/d(v_j), d(v_i)/d(v_j)\n",
    "                F[i, j+3] += 0.5 * term1 * dt**2\n",
    "                F[i+3, j+3] += term1 * dt\n",
    "    return F\n",
    "\n",
    "# (h_measurement_function, calculate_jacobian_Hj, detect_object_huggingface 함수는 이전과 동일)\n",
    "def h_measurement_function(x_state_9d):\n",
    "    cx, cy, Z = x_state_9d[0], x_state_9d[1], x_state_9d[2]\n",
    "    Z = np.clip(Z, 0.1, 1000); w_pred = (FOCAL_LENGTH_PIXELS*REAL_OBJECT_WIDTH_M)/Z\n",
    "    h_pred = (FOCAL_LENGTH_PIXELS*REAL_OBJECT_HEIGHT_M)/Z; return np.array([cx, cy, w_pred, h_pred])\n",
    "def calculate_jacobian_Hj(x_eval_9d):\n",
    "    Z = np.clip(x_eval_9d[2], 0.1, 1000); H_j = np.zeros((4, 9)); H_j[0, 0]=1; H_j[1, 1]=1\n",
    "    H_j[2, 2]=-(FOCAL_LENGTH_PIXELS*REAL_OBJECT_WIDTH_M)/(Z**2)\n",
    "    H_j[3, 2]=-(FOCAL_LENGTH_PIXELS*REAL_OBJECT_HEIGHT_M)/(Z**2); return H_j\n",
    "def detect_object_huggingface(frame_color_cv):\n",
    "    image_pil=Image.fromarray(cv2.cvtColor(frame_color_cv,cv2.COLOR_BGR2RGB)); inputs=image_processor(images=image_pil,return_tensors=\"pt\")\n",
    "    with torch.no_grad(): outputs=model(**inputs)\n",
    "    target_sizes=torch.tensor([image_pil.size[::-1]]); results=image_processor.post_process_object_detection(outputs,threshold=CONFIDENCE_THRESHOLD,target_sizes=target_sizes)[0]\n",
    "    best_target_detection=None\n",
    "    for s,l,b in zip(results[\"scores\"],results[\"labels\"],results[\"boxes\"]):\n",
    "        if model.config.id2label[l.item()].lower()==USER_TARGET_CLASS_NAME:\n",
    "            b=[round(i,2) for i in b.tolist()]; w,h=b[2]-b[0],b[3]-b[1]; cx,cy=b[0]+w/2,b[1]+h/2\n",
    "            if best_target_detection is None or s.item()>best_target_detection[0]: best_target_detection=(s.item(),np.array([cx,cy,w,h]))\n",
    "    return best_target_detection[1] if best_target_detection else None\n",
    "\n",
    "# ==============================================================\n",
    "# 단계 1: 비디오 및 EKF 초기화\n",
    "# ==============================================================\n",
    "print(\"단계 1: 비디오 및 EKF 초기화 중...\")\n",
    "cap = cv2.VideoCapture(VIDEO_INPUT_PATH)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) if cap.get(cv2.CAP_PROP_FPS) > 0 else 30\n",
    "dt = 1.0 / fps\n",
    "frame_width, frame_height = int(cap.get(3)), int(cap.get(4))\n",
    "out_video = cv2.VideoWriter(VIDEO_OUTPUT_PATH, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "x_est = np.zeros(9); P = np.eye(9) * 1000.0; is_initialized = False\n",
    "print(\"단계 1 완료.\")\n",
    "\n",
    "# ==============================================================\n",
    "# 단계 2: 영상 처리 및 EKF 적용 루프\n",
    "# ==============================================================\n",
    "print(\"단계 2: 영상 처리 및 EKF 적용 시작...\")\n",
    "frame_idx = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "    output_frame = frame.copy()\n",
    "    \n",
    "    measurement_k = detect_object_huggingface(frame)\n",
    "\n",
    "    if not is_initialized and measurement_k is not None:\n",
    "        cx_meas, cy_meas, w_meas, h_meas = measurement_k\n",
    "        if h_meas > 1:\n",
    "            initial_Z = (FOCAL_LENGTH_PIXELS * REAL_OBJECT_HEIGHT_M) / h_meas\n",
    "            x_est[0], x_est[1], x_est[2] = cx_meas, cy_meas, initial_Z\n",
    "            P = np.eye(9) * 100.0; is_initialized = True\n",
    "            print(f\"프레임 {frame_idx}에서 EKF 초기화. 초기 깊이 Z = {initial_Z:.2f} m\")\n",
    "\n",
    "    if is_initialized:\n",
    "        # <<< [DRAG] EKF 예측 단계 수정 >>>\n",
    "        F_k = calculate_jacobian_F(x_est, dt, K_DRAG)\n",
    "        x_pred = state_transition_function_f(x_est, dt, K_DRAG)\n",
    "        P_pred = F_k @ P @ F_k.T + Q\n",
    "\n",
    "        # EKF 갱신 단계 (이전과 동일)\n",
    "        if measurement_k is not None:\n",
    "            H_j_k = calculate_jacobian_Hj(x_pred)\n",
    "            z_pred = h_measurement_function(x_pred)\n",
    "            y = measurement_k - z_pred\n",
    "            S = H_j_k @ P_pred @ H_j_k.T + R\n",
    "            K = P_pred @ H_j_k.T @ np.linalg.inv(S)\n",
    "            x_est = x_pred + K @ y\n",
    "            P = (np.eye(9) - K @ H_j_k) @ P_pred\n",
    "        else:\n",
    "            x_est = x_pred\n",
    "            P = P_pred\n",
    "\n",
    "    # 시각화 (이전과 동일)\n",
    "    if measurement_k is not None:\n",
    "        cx,cy,w,h=measurement_k; cv2.rectangle(output_frame,(int(cx-w/2),int(cy-h/2)),(int(cx+w/2),int(cy+h/2)),(0,255,0),1)\n",
    "    if is_initialized:\n",
    "        est_box=h_measurement_function(x_est); cx,cy,w,h=est_box; est_Z=x_est[2]; est_a=x_est[6:9]\n",
    "        cv2.rectangle(output_frame,(int(cx-w/2),int(cy-h/2)),(int(cx+w/2),int(cy+h/2)),(0,0,255),2)\n",
    "        info1=f\"EKF Z:{est_Z:.1f}m\"; info2=f\"a(px/s2):({int(est_a[0])},{int(est_a[1])}) aZ:{est_a[2]:.1f}\"\n",
    "        cv2.putText(output_frame,info1,(int(cx-w/2),int(cy-h/2)-25),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,255),2)\n",
    "        cv2.putText(output_frame,info2,(int(cx-w/2),int(cy-h/2)-10),cv2.FONT_HERSHEY_SIMPLEX,0.4,(0,0,255),1)\n",
    "\n",
    "    out_video.write(output_frame)\n",
    "    frame_idx += 1\n",
    "    if frame_idx % 30 == 0: print(f\"프레임 {frame_idx} 처리 중...\")\n",
    "\n",
    "# 단계 4: 종료\n",
    "print(\"단계 4: 처리 완료 및 종료.\")\n",
    "cap.release(), out_video.release(), cv2.destroyAllWindows()\n",
    "print(f\"공기 저항 모델 EKF 처리 완료. 결과: {VIDEO_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd1d7572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face 모델 로드 성공.\n",
      "단계 1: 비디오 및 선형 칼만 필터 초기화 중...\n",
      "단계 1 완료.\n",
      "단계 2: 영상 처리 및 선형 KF 적용 시작...\n",
      "프레임 0에서 선형 KF 초기화.\n",
      "프레임 30 처리 중...\n",
      "프레임 60 처리 중...\n",
      "프레임 90 처리 중...\n",
      "프레임 120 처리 중...\n",
      "프레임 150 처리 중...\n",
      "프레임 180 처리 중...\n",
      "프레임 210 처리 중...\n",
      "프레임 240 처리 중...\n",
      "프레임 270 처리 중...\n",
      "프레임 300 처리 중...\n",
      "프레임 330 처리 중...\n",
      "프레임 360 처리 중...\n",
      "프레임 390 처리 중...\n",
      "프레임 420 처리 중...\n",
      "프레임 450 처리 중...\n",
      "프레임 480 처리 중...\n",
      "프레임 510 처리 중...\n",
      "프레임 540 처리 중...\n",
      "프레임 570 처리 중...\n",
      "프레임 600 처리 중...\n",
      "프레임 630 처리 중...\n",
      "프레임 660 처리 중...\n",
      "프레임 690 처리 중...\n",
      "프레임 720 처리 중...\n",
      "프레임 750 처리 중...\n",
      "프레임 780 처리 중...\n",
      "단계 4: 처리 완료 및 종료.\n",
      "선형 KF 처리 완료. 결과: EKF_vid2_o2.mp4\n"
     ]
    }
   ],
   "source": [
    "# 선형 바운딩 등가속도 모델\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from transformers import AutoImageProcessor, AutoModelForObjectDetection\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# --- 설정 변수 ---\n",
    "VIDEO_INPUT_PATH = \"KF_vid2.mp4\" # 분석한 영상 파일명으로 변경\n",
    "VIDEO_OUTPUT_PATH = \"EKF_vid2_o2.mp4\"\n",
    "USER_TARGET_CLASS_NAME = 'sports ball'\n",
    "HF_MODEL_NAME = \"facebook/detr-resnet-50\"\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "\n",
    "# --- 시스템 상태 정의 (12-DOF Linear) ---\n",
    "# <<< [LINEAR] 상태 변수가 12차원으로 확장: [cx, cy, w, h, v_cx, v_cy, v_w, v_h, a_cx, a_cy, a_w, a_h] >>>\n",
    "# <<< [LINEAR] 물리 파라미터(초점거리, 실제크기)는 더 이상 필요 없음 >>>\n",
    "\n",
    "# Q: 프로세스 노이즈. 가속도의 불확실성(Jerk).\n",
    "q_accel_pos_std = 10.0  # 픽셀 위치 가속도 변화의 불확실성\n",
    "q_accel_size_std = 10.0 # 픽셀 크기 가속도 변화의 불확실성\n",
    "Q = np.diag([0,0,0,0, 0,0,0,0, q_accel_pos_std**2, q_accel_pos_std**2, q_accel_size_std**2, q_accel_size_std**2])\n",
    "\n",
    "# R: 측정 노이즈.\n",
    "r_pos_std = 3.0\n",
    "r_size_std = 5.0\n",
    "R = np.diag([r_pos_std**2, r_pos_std**2, r_size_std**2, r_size_std**2])\n",
    "\n",
    "# --- Hugging Face 모델 로드 (변경 없음) ---\n",
    "try:\n",
    "    image_processor = AutoImageProcessor.from_pretrained(HF_MODEL_NAME)\n",
    "    model = AutoModelForObjectDetection.from_pretrained(HF_MODEL_NAME)\n",
    "    print(\"Hugging Face 모델 로드 성공.\")\n",
    "except Exception as e: exit(f\"모델 로드 실패: {e}\")\n",
    "\n",
    "# (객체 탐지 함수는 변경 없음)\n",
    "def detect_object_huggingface(frame_color_cv):\n",
    "    image_pil = Image.fromarray(cv2.cvtColor(frame_color_cv, cv2.COLOR_BGR2RGB))\n",
    "    inputs = image_processor(images=image_pil, return_tensors=\"pt\")\n",
    "    with torch.no_grad(): outputs = model(**inputs)\n",
    "    target_sizes = torch.tensor([image_pil.size[::-1]])\n",
    "    results = image_processor.post_process_object_detection(outputs, threshold=CONFIDENCE_THRESHOLD, target_sizes=target_sizes)[0]\n",
    "    best_target_detection = None\n",
    "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "        if model.config.id2label[label.item()].lower() == USER_TARGET_CLASS_NAME:\n",
    "            box = [round(i, 2) for i in box.tolist()]\n",
    "            w, h = box[2] - box[0], box[3] - box[1]\n",
    "            cx, cy = box[0] + w/2, box[1] + h/2\n",
    "            current_score = score.item()\n",
    "            if best_target_detection is None or current_score > best_target_detection[0]:\n",
    "                 best_target_detection = (current_score, np.array([cx, cy, w, h]))\n",
    "    return best_target_detection[1] if best_target_detection else None\n",
    "\n",
    "# ==============================================================\n",
    "# 단계 1: 비디오 및 칼만 필터 초기화\n",
    "# ==============================================================\n",
    "print(\"단계 1: 비디오 및 선형 칼만 필터 초기화 중...\")\n",
    "cap = cv2.VideoCapture(VIDEO_INPUT_PATH)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) if cap.get(cv2.CAP_PROP_FPS) > 0 else 30\n",
    "dt = 1.0 / fps\n",
    "frame_width, frame_height = int(cap.get(3)), int(cap.get(4))\n",
    "out_video = cv2.VideoWriter(VIDEO_OUTPUT_PATH, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "# <<< [LINEAR] 상태 전이 행렬 A를 12x12로 정의 >>>\n",
    "A = np.eye(12)\n",
    "for i in range(4):\n",
    "    A[i, i+4] = dt\n",
    "    A[i, i+8] = 0.5 * dt**2\n",
    "    A[i+4, i+8] = dt\n",
    "\n",
    "# <<< [LINEAR] 측정 행렬 H를 4x12 상수로 정의 (자코비안 불필요) >>>\n",
    "# 측정값 [cx, cy, w, h]는 상태 벡터의 첫 4개 요소에 해당\n",
    "H = np.zeros((4, 12))\n",
    "H[0, 0] = 1\n",
    "H[1, 1] = 1\n",
    "H[2, 2] = 1\n",
    "H[3, 3] = 1\n",
    "\n",
    "x_est = np.zeros(12) # 12차원 상태 벡터\n",
    "P = np.eye(12) * 1000.0 # 12x12 공분산 행렬\n",
    "is_initialized = False\n",
    "print(\"단계 1 완료.\")\n",
    "\n",
    "# ==============================================================\n",
    "# 단계 2: 영상 처리 및 선형 KF 적용 루프\n",
    "# ==============================================================\n",
    "print(\"단계 2: 영상 처리 및 선형 KF 적용 시작...\")\n",
    "frame_idx = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "    output_frame = frame.copy()\n",
    "    \n",
    "    measurement_k = detect_object_huggingface(frame)\n",
    "\n",
    "    if not is_initialized and measurement_k is not None:\n",
    "        # <<< [LINEAR] 12차원 상태 벡터 초기화 >>>\n",
    "        x_est[0:4] = measurement_k # 위치(cx,cy,w,h)는 측정값으로 초기화\n",
    "        # 속도, 가속도는 0으로 시작\n",
    "        P = np.eye(12) * 100.0\n",
    "        is_initialized = True\n",
    "        print(f\"프레임 {frame_idx}에서 선형 KF 초기화.\")\n",
    "\n",
    "    if is_initialized:\n",
    "        # 예측 단계\n",
    "        x_pred = A @ x_est\n",
    "        P_pred = A @ P @ A.T + Q\n",
    "\n",
    "        # 갱신 단계\n",
    "        if measurement_k is not None:\n",
    "            # <<< [LINEAR] 표준 선형 칼만 필터 업데이트 공식 사용 >>>\n",
    "            y = measurement_k - H @ x_pred\n",
    "            S = H @ P_pred @ H.T + R\n",
    "            K = P_pred @ H.T @ np.linalg.inv(S)\n",
    "            x_est = x_pred + K @ y\n",
    "            P = (np.eye(12) - K @ H) @ P_pred\n",
    "        else:\n",
    "            x_est = x_pred\n",
    "            P = P_pred\n",
    "\n",
    "    # ==============================================================\n",
    "    # 단계 3: 시각화\n",
    "    # ==============================================================\n",
    "    if measurement_k is not None:\n",
    "        cx, cy, w, h = measurement_k\n",
    "        cv2.rectangle(output_frame, (int(cx-w/2), int(cy-h/2)), (int(cx+w/2), int(cy+h/2)), (0, 255, 0), 1)\n",
    "\n",
    "    if is_initialized:\n",
    "        # <<< [LINEAR] 상태 벡터에서 직접 cx,cy,w,h 값을 가져옴 >>>\n",
    "        cx, cy, w, h = x_est[0], x_est[1], x_est[2], x_est[3]\n",
    "        cv2.rectangle(output_frame, (int(cx-w/2), int(cy-h/2)), (int(cx+w/2), int(cy+h/2)), (0, 0, 255), 2)\n",
    "        cv2.putText(output_frame, \"Linear KF\", (int(cx-w/2), int(cy-h/2)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "\n",
    "    out_video.write(output_frame)\n",
    "    frame_idx += 1\n",
    "    if frame_idx % 30 == 0: print(f\"프레임 {frame_idx} 처리 중...\")\n",
    "\n",
    "# ==============================================================\n",
    "# 단계 4: 종료\n",
    "# ==============================================================\n",
    "print(\"단계 4: 처리 완료 및 종료.\")\n",
    "cap.release(), out_video.release(), cv2.destroyAllWindows()\n",
    "print(f\"선형 KF 처리 완료. 결과: {VIDEO_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6104766d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face 모델 로드 성공.\n",
      "단계 1: 비디오 및 선형 칼만 필터 초기화 중...\n",
      "단계 1 완료.\n",
      "단계 2: 영상 처리 및 선형 KF 적용 시작...\n",
      "프레임 4에서 선형 KF 초기화.\n",
      "프레임 30 처리 중...\n",
      "프레임 60 처리 중...\n",
      "프레임 90 처리 중...\n",
      "프레임 120 처리 중...\n",
      "프레임 150 처리 중...\n",
      "프레임 180 처리 중...\n",
      "프레임 210 처리 중...\n",
      "프레임 240 처리 중...\n",
      "프레임 270 처리 중...\n",
      "프레임 300 처리 중...\n",
      "프레임 330 처리 중...\n",
      "프레임 360 처리 중...\n",
      "프레임 390 처리 중...\n",
      "단계 4: 처리 완료 및 종료.\n",
      "선형 등속도 모델 처리 완료. 결과: bowling_o.mp4\n"
     ]
    }
   ],
   "source": [
    "# 선형 바운딩 등속도 모델\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from transformers import AutoImageProcessor, AutoModelForObjectDetection\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# --- 설정 변수 ---\n",
    "VIDEO_INPUT_PATH = \"bowling.mp4\"\n",
    "VIDEO_OUTPUT_PATH = \"bowling_o.mp4\"\n",
    "USER_TARGET_CLASS_NAME = 'sports ball'\n",
    "HF_MODEL_NAME = \"facebook/detr-resnet-50\"\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "\n",
    "# --- 시스템 상태 정의 (8-DOF Linear CV) ---\n",
    "# <<< [CV MODEL] 상태 변수: [cx, cy, w, h, v_cx, v_cy, v_w, v_h] >>>\n",
    "# <<< [CV MODEL] 물리 파라미터 및 가속도 항 모두 제거 >>>\n",
    "\n",
    "# Q: 프로세스 노이즈. 등속도 모델의 불확실성 (즉, 모델링되지 않은 가속도).\n",
    "# 이 값을 키우면 반응성이, 줄이면 안정성이 높아짐.\n",
    "q_vel_pos_std = 5.0  # 픽셀 위치 속도의 불확실성\n",
    "q_vel_size_std = 5.0 # 픽셀 크기 속도의 불확실성\n",
    "Q = np.diag([0, 0, 0, 0, q_vel_pos_std**2, q_vel_pos_std**2, q_vel_size_std**2, q_vel_size_std**2])\n",
    "\n",
    "# R: 측정 노이즈.\n",
    "r_pos_std = 3.0\n",
    "r_size_std = 5.0\n",
    "R = np.diag([r_pos_std**2, r_pos_std**2, r_size_std**2, r_size_std**2])\n",
    "\n",
    "# --- Hugging Face 모델 로드 (변경 없음) ---\n",
    "try:\n",
    "    image_processor = AutoImageProcessor.from_pretrained(HF_MODEL_NAME)\n",
    "    model = AutoModelForObjectDetection.from_pretrained(HF_MODEL_NAME)\n",
    "    print(\"Hugging Face 모델 로드 성공.\")\n",
    "except Exception as e: exit(f\"모델 로드 실패: {e}\")\n",
    "\n",
    "# (객체 탐지 함수는 변경 없음)\n",
    "def detect_object_huggingface(frame_color_cv):\n",
    "    image_pil = Image.fromarray(cv2.cvtColor(frame_color_cv, cv2.COLOR_BGR2RGB))\n",
    "    inputs = image_processor(images=image_pil, return_tensors=\"pt\")\n",
    "    with torch.no_grad(): outputs = model(**inputs)\n",
    "    target_sizes = torch.tensor([image_pil.size[::-1]])\n",
    "    results = image_processor.post_process_object_detection(outputs, threshold=CONFIDENCE_THRESHOLD, target_sizes=target_sizes)[0]\n",
    "    best_target_detection = None\n",
    "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "        if model.config.id2label[label.item()].lower() == USER_TARGET_CLASS_NAME:\n",
    "            box = [round(i, 2) for i in box.tolist()]\n",
    "            w, h = box[2] - box[0], box[3] - box[1]\n",
    "            cx, cy = box[0] + w/2, box[1] + h/2\n",
    "            current_score = score.item()\n",
    "            if best_target_detection is None or current_score > best_target_detection[0]:\n",
    "                 best_target_detection = (current_score, np.array([cx, cy, w, h]))\n",
    "    return best_target_detection[1] if best_target_detection else None\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# 단계 1: 비디오 및 칼만 필터 초기화\n",
    "# ==============================================================\n",
    "print(\"단계 1: 비디오 및 선형 칼만 필터 초기화 중...\")\n",
    "cap = cv2.VideoCapture(VIDEO_INPUT_PATH)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) if cap.get(cv2.CAP_PROP_FPS) > 0 else 30\n",
    "dt = 1.0 / fps\n",
    "frame_width, frame_height = int(cap.get(3)), int(cap.get(4))\n",
    "out_video = cv2.VideoWriter(VIDEO_OUTPUT_PATH, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "# <<< [CV MODEL] 상태 전이 행렬 A를 8x8로 정의 >>>\n",
    "A = np.eye(8)\n",
    "for i in range(4):\n",
    "    A[i, i+4] = dt\n",
    "\n",
    "# <<< [CV MODEL] 측정 행렬 H를 4x8 상수로 정의 >>>\n",
    "H = np.zeros((4, 8))\n",
    "for i in range(4):\n",
    "    H[i, i] = 1\n",
    "\n",
    "x_est = np.zeros(8) # 8차원 상태 벡터\n",
    "P = np.eye(8) * 1000.0 # 8x8 공분산 행렬\n",
    "is_initialized = False\n",
    "print(\"단계 1 완료.\")\n",
    "\n",
    "# ==============================================================\n",
    "# 단계 2: 영상 처리 및 선형 KF 적용 루프\n",
    "# ==============================================================\n",
    "print(\"단계 2: 영상 처리 및 선형 KF 적용 시작...\")\n",
    "frame_idx = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "    output_frame = frame.copy()\n",
    "    \n",
    "    measurement_k = detect_object_huggingface(frame)\n",
    "\n",
    "    if not is_initialized and measurement_k is not None:\n",
    "        # <<< [CV MODEL] 8차원 상태 벡터 초기화 >>>\n",
    "        x_est[0:4] = measurement_k # 위치(cx,cy,w,h)는 측정값으로 초기화\n",
    "        # 속도는 0으로 시작\n",
    "        P = np.eye(8) * 100.0\n",
    "        is_initialized = True\n",
    "        print(f\"프레임 {frame_idx}에서 선형 KF 초기화.\")\n",
    "\n",
    "    if is_initialized:\n",
    "        # 예측 단계\n",
    "        x_pred = A @ x_est\n",
    "        P_pred = A @ P @ A.T + Q\n",
    "\n",
    "        # 갱신 단계\n",
    "        if measurement_k is not None:\n",
    "            # <<< [CV MODEL] 표준 선형 칼만 필터 업데이트 공식 사용 >>>\n",
    "            y = measurement_k - H @ x_pred\n",
    "            S = H @ P_pred @ H.T + R\n",
    "            K = P_pred @ H.T @ np.linalg.inv(S)\n",
    "            x_est = x_pred + K @ y\n",
    "            P = (np.eye(8) - K @ H) @ P_pred\n",
    "        else:\n",
    "            x_est = x_pred\n",
    "            P = P_pred\n",
    "\n",
    "    # ==============================================================\n",
    "    # 단계 3: 시각화\n",
    "    # ==============================================================\n",
    "    if measurement_k is not None:\n",
    "        cx, cy, w, h = measurement_k\n",
    "        cv2.rectangle(output_frame, (int(cx-w/2), int(cy-h/2)), (int(cx+w/2), int(cy+h/2)), (0, 255, 0), 1)\n",
    "\n",
    "    if is_initialized:\n",
    "        cx, cy, w, h = x_est[0], x_est[1], x_est[2], x_est[3]\n",
    "        cv2.rectangle(output_frame, (int(cx-w/2), int(cy-h/2)), (int(cx+w/2), int(cy+h/2)), (0, 0, 255), 2)\n",
    "        cv2.putText(output_frame, \"Linear CV Model\", (int(cx-w/2), int(cy-h/2)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "\n",
    "    out_video.write(output_frame)\n",
    "    frame_idx += 1\n",
    "    if frame_idx % 30 == 0: print(f\"프레임 {frame_idx} 처리 중...\")\n",
    "\n",
    "# ==============================================================\n",
    "# 단계 4: 종료\n",
    "# ==============================================================\n",
    "print(\"단계 4: 처리 완료 및 종료.\")\n",
    "cap.release(), out_video.release(), cv2.destroyAllWindows()\n",
    "print(f\"선형 등속도 모델 처리 완료. 결과: {VIDEO_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14c20b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face 모델 로드 성공.\n",
      "단계 1: 비디오 및 EKF 초기화 중...\n",
      "단계 1 완료.\n",
      "단계 2: 영상 처리 및 EKF 적용 시작...\n",
      "프레임 4에서 EKF 초기화. 추정된 초기 깊이 Z = 3.13 m\n",
      "프레임 30 처리 중...\n",
      "프레임 60 처리 중...\n",
      "프레임 90 처리 중...\n",
      "프레임 120 처리 중...\n",
      "프레임 150 처리 중...\n",
      "프레임 180 처리 중...\n",
      "프레임 210 처리 중...\n",
      "프레임 240 처리 중...\n",
      "프레임 270 처리 중...\n",
      "프레임 300 처리 중...\n",
      "프레임 330 처리 중...\n",
      "프레임 360 처리 중...\n",
      "프레임 390 처리 중...\n",
      "단계 4: 처리 완료 및 종료.\n",
      "오류: 로그 파일을 저장할 수 없습니다.\n"
     ]
    }
   ],
   "source": [
    "# 핀홀 카메라 등속도 모델 (로그 기록 기능 추가)\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from transformers import AutoImageProcessor, AutoModelForObjectDetection\n",
    "import torch\n",
    "from PIL import Image\n",
    "import csv  # <<< [LOG] CSV 라이브러리 추가\n",
    "\n",
    "# --- 설정 변수 ---\n",
    "VIDEO_INPUT_PATH = \"bowling.mp4\"\n",
    "VIDEO_OUTPUT_PATH = \"bowling_P_o_log.mp4\"\n",
    "USER_TARGET_CLASS_NAME = 'sports ball'\n",
    "HF_MODEL_NAME = \"facebook/detr-resnet-50\"\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "\n",
    "# --- EKF를 위한 물리 파라미터 ---\n",
    "FOCAL_LENGTH_PIXELS = 1200\n",
    "REAL_OBJECT_WIDTH_M = 0.22\n",
    "REAL_OBJECT_HEIGHT_M = 0.22\n",
    "\n",
    "# --- 시스템 상태 정의 ---\n",
    "q_pos_std = 5.0\n",
    "q_depth_std = 5.0\n",
    "Q = np.diag([0, 0, 0, q_pos_std**2, q_pos_std**2, q_depth_std**2])\n",
    "r_pos_std = 3.0\n",
    "r_size_std = 5.0\n",
    "R = np.diag([r_pos_std**2, r_pos_std**2, r_size_std**2, r_size_std**2])\n",
    "\n",
    "# --- Hugging Face 모델 로드 ---\n",
    "try:\n",
    "    image_processor = AutoImageProcessor.from_pretrained(HF_MODEL_NAME)\n",
    "    model = AutoModelForObjectDetection.from_pretrained(HF_MODEL_NAME)\n",
    "    print(\"Hugging Face 모델 로드 성공.\")\n",
    "except Exception as e: exit(f\"모델 로드 실패: {e}\")\n",
    "\n",
    "# --- EKF 관련 함수 정의 (변경 없음) ---\n",
    "def h_measurement_function(x_state):\n",
    "    cx, cy, Z, _, _, _ = x_state\n",
    "    Z = np.clip(Z, 0.1, 1000)\n",
    "    w_pred = (FOCAL_LENGTH_PIXELS * REAL_OBJECT_WIDTH_M) / Z\n",
    "    h_pred = (FOCAL_LENGTH_PIXELS * REAL_OBJECT_HEIGHT_M) / Z\n",
    "    return np.array([cx, cy, w_pred, h_pred])\n",
    "def calculate_jacobian_Hj(x_eval):\n",
    "    _, _, Z, _, _, _ = x_eval\n",
    "    Z = np.clip(Z, 0.1, 1000)\n",
    "    H_j = np.zeros((4, 6)); H_j[0, 0] = 1; H_j[1, 1] = 1\n",
    "    H_j[2, 2] = -(FOCAL_LENGTH_PIXELS * REAL_OBJECT_WIDTH_M) / (Z**2)\n",
    "    H_j[3, 2] = -(FOCAL_LENGTH_PIXELS * REAL_OBJECT_HEIGHT_M) / (Z**2)\n",
    "    return H_j\n",
    "def detect_object_huggingface(frame_color_cv):\n",
    "    image_pil = Image.fromarray(cv2.cvtColor(frame_color_cv, cv2.COLOR_BGR2RGB))\n",
    "    inputs = image_processor(images=image_pil, return_tensors=\"pt\")\n",
    "    with torch.no_grad(): outputs = model(**inputs)\n",
    "    target_sizes = torch.tensor([image_pil.size[::-1]])\n",
    "    results = image_processor.post_process_object_detection(outputs, threshold=CONFIDENCE_THRESHOLD, target_sizes=target_sizes)[0]\n",
    "    best_target_detection = None\n",
    "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "        if model.config.id2label[label.item()].lower() == USER_TARGET_CLASS_NAME:\n",
    "            box=[round(i,2) for i in box.tolist()]; w,h=box[2]-box[0],box[3]-box[1]; cx,cy=box[0]+w/2,box[1]+h/2\n",
    "            if best_target_detection is None or score.item() > best_target_detection[0]:\n",
    "                 best_target_detection = (score.item(), np.array([cx, cy, w, h]))\n",
    "    return best_target_detection[1] if best_target_detection else None\n",
    "\n",
    "# ==============================================================\n",
    "# 단계 1: 비디오 및 EKF 초기화\n",
    "# ==============================================================\n",
    "print(\"단계 1: 비디오 및 EKF 초기화 중...\")\n",
    "cap = cv2.VideoCapture(VIDEO_INPUT_PATH)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) if cap.get(cv2.CAP_PROP_FPS) > 0 else 30\n",
    "dt = 1.0 / fps\n",
    "frame_width, frame_height = int(cap.get(3)), int(cap.get(4))\n",
    "out_video = cv2.VideoWriter(VIDEO_OUTPUT_PATH, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "A = np.eye(6); A[0, 3], A[1, 4], A[2, 5] = dt, dt, dt\n",
    "x_est = np.zeros(6); P = np.eye(6) * 1000.0; is_initialized = False\n",
    "\n",
    "log_data = [] # <<< [LOG] 로그 데이터를 저장할 리스트 초기화\n",
    "log_header = [ # <<< [LOG] CSV 파일의 헤더 정의\n",
    "    'frame', 'is_detected',\n",
    "    'pos_cx', 'pos_cy', 'pos_Z',\n",
    "    'vel_cx', 'vel_cy', 'vel_Z',\n",
    "    'P_diag_cx', 'P_diag_cy', 'P_diag_Z',\n",
    "    'P_diag_vcx', 'P_diag_vcy', 'P_diag_vZ'\n",
    "]\n",
    "\n",
    "print(\"단계 1 완료.\")\n",
    "# ==============================================================\n",
    "# 단계 2: 영상 처리 및 EKF 적용 루프\n",
    "# ==============================================================\n",
    "print(\"단계 2: 영상 처리 및 EKF 적용 시작...\")\n",
    "frame_idx = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "    output_frame = frame.copy()\n",
    "    \n",
    "    measurement_k = detect_object_huggingface(frame)\n",
    "\n",
    "    if not is_initialized and measurement_k is not None:\n",
    "        if measurement_k[3] > 1:\n",
    "            initial_Z = (FOCAL_LENGTH_PIXELS * REAL_OBJECT_HEIGHT_M) / measurement_k[3]\n",
    "            x_est[0], x_est[1], x_est[2] = measurement_k[0], measurement_k[1], initial_Z\n",
    "            P = np.eye(6) * 100.0; is_initialized = True\n",
    "            print(f\"프레임 {frame_idx}에서 EKF 초기화. 추정된 초기 깊이 Z = {initial_Z:.2f} m\")\n",
    "\n",
    "    if is_initialized:\n",
    "        x_pred = A @ x_est\n",
    "        P_pred = A @ P @ A.T + Q\n",
    "\n",
    "        if measurement_k is not None:\n",
    "            H_j_k = calculate_jacobian_Hj(x_pred)\n",
    "            z_pred = h_measurement_function(x_pred)\n",
    "            y = measurement_k - z_pred\n",
    "            S = H_j_k @ P_pred @ H_j_k.T + R\n",
    "            K = P_pred @ H_j_k.T @ np.linalg.inv(S)\n",
    "            x_est = x_pred + K @ y\n",
    "            P = (np.eye(6) - K @ H_j_k) @ P_pred\n",
    "        else:\n",
    "            x_est = x_pred\n",
    "            P = P_pred\n",
    "\n",
    "    # ==============================================================\n",
    "    # 단계 3.5: 로그 기록\n",
    "    # ==============================================================\n",
    "    if is_initialized:\n",
    "        P_diag = np.diag(P)\n",
    "        current_log = {\n",
    "            'frame': frame_idx,\n",
    "            'is_detected': 1 if measurement_k is not None else 0,\n",
    "            'pos_cx': x_est[0], 'pos_cy': x_est[1], 'pos_Z': x_est[2],\n",
    "            'vel_cx': x_est[3], 'vel_cy': x_est[4], 'vel_Z': x_est[5],\n",
    "            'P_diag_cx': P_diag[0], 'P_diag_cy': P_diag[1], 'P_diag_Z': P_diag[2],\n",
    "            'P_diag_vcx': P_diag[3], 'P_diag_vcy': P_diag[4], 'P_diag_vZ': P_diag[5]\n",
    "        }\n",
    "        log_data.append(current_log)\n",
    "\n",
    "    # (시각화 단계는 변경 없음)\n",
    "    if measurement_k is not None:\n",
    "        cx,cy,w,h=measurement_k; cv2.rectangle(output_frame,(int(cx-w/2),int(cy-h/2)),(int(cx+w/2),int(cy+h/2)),(0,255,0),2)\n",
    "        cv2.putText(output_frame,\"Detection\",(int(cx-w/2),int(cy-h/2)-10),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,255,0),1)\n",
    "    if is_initialized:\n",
    "        est_box=h_measurement_function(x_est); cx,cy,w,h=est_box; est_Z=x_est[2]\n",
    "        cv2.rectangle(output_frame,(int(cx-w/2),int(cy-h/2)),(int(cx+w/2),int(cy+h/2)),(0,0,255),2)\n",
    "        cv2.putText(output_frame,f\"EKF (Z:{est_Z:.1f}m)\",(int(cx-w/2),int(cy-h/2)-10),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,255),2)\n",
    "\n",
    "    out_video.write(output_frame)\n",
    "    frame_idx += 1\n",
    "    if frame_idx % 30 == 0: print(f\"프레임 {frame_idx} 처리 중...\")\n",
    "\n",
    "# ==============================================================\n",
    "# 단계 4: 종료\n",
    "# ==============================================================\n",
    "print(\"단계 4: 처리 완료 및 종료.\")\n",
    "cap.release()\n",
    "out_video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# ==============================================================\n",
    "# 단계 5: 로그 파일 저장\n",
    "# ==============================================================\n",
    "try:\n",
    "    with open('log.csv', 'w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=log_header)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(log_data)\n",
    "    print(f\"로그 데이터 저장 완료. -> log.csv\")\n",
    "except IOError:\n",
    "    print(\"오류: 로그 파일을 저장할 수 없습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
